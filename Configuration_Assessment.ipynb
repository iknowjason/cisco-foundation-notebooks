{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI5ozM2h2Nva"
   },
   "source": [
    "# Use Case Description\n",
    "\n",
    "In nearly every security incident, misconfiguration is either the root cause or a critical enabler. Today, most config assessments rely on static rule-matching engines that check config files or cloud settings against predefined rules. But these tools lack contextual understanding of how different components interact — they can't tell if a setting is safe for your environment. This use case focuses on an AI model that ingests configuration data across cloud, infra, app, and identity layers — and reasons over them to answer:\n",
    "\n",
    "## Model used for this use case\n",
    "Both Instruct Model and Reasoning Model would be suitable for this task. In this example, we used Reasoning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetUp\n",
    "\n",
    "The setup scripts below are essentially the same as those in the [Quickstart (Reasoning Model)](https://github.com/RobustIntelligence/foundation-ai-cookbook/blob/main/1_quickstarts/Preview_Quickstart_reasoning_model.ipynb)\n",
    "\n",
    "### Notice\n",
    "- The code below assumes that users have access to the models via Hugging Face. If you are using API access instead, please replace the inference code with the API version provided in the Quickstart guide.\n",
    "- This model is currently in preview mode and may receive updates. As a result, outputs can vary even when parameters are configured to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BPloud0h2LI9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "def _get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = _get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u5pjZGi32FsI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6843a52f185f4971a1425e928865b88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"\" # To be relaced with the final model name\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32, # this model's tensor_type is float32\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def inference(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # The old model version didn’t include the <think> token in the chat template.\n",
    "    think_token = \"<think>\\n\"\n",
    "    if not inputs.endswith(think_token):\n",
    "        inputs += think_token\n",
    "    \n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_args,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens = False)\n",
    "\n",
    "    # extract the thinking part only\n",
    "    match = re.search(r\"<think>(.*?)<\\|end_of_text\\|>\", response, re.DOTALL)\n",
    "    \n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2XV1flWD-qM"
   },
   "source": [
    "## Config Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY5Wyux1EhVQ"
   },
   "source": [
    "Here’s a mock configuration for a Linux-based SSH server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Obe30-VYESs6"
   },
   "outputs": [],
   "source": [
    "config_text = \"\"\"\n",
    "provider \"google\" {\n",
    "  project = \"my-project\"\n",
    "  region  = \"us-central1\"\n",
    "}\n",
    "\n",
    "resource \"google_compute_network\" \"default\" {\n",
    "  name                    = \"default-network\"\n",
    "  auto_create_subnetworks = true\n",
    "}\n",
    "\n",
    "resource \"google_compute_firewall\" \"allow_all_inbound\" {\n",
    "  name    = \"allow-all-inbound\"\n",
    "  network = google_compute_network.default.name\n",
    "\n",
    "  direction = \"INGRESS\"\n",
    "  priority  = 1000\n",
    "\n",
    "  allows {\n",
    "    protocol = \"tcp\"\n",
    "    ports    = [\"0-65535\"]\n",
    "  }\n",
    "\n",
    "  allows {\n",
    "    protocol = \"udp\"\n",
    "    ports    = [\"0-65535\"]\n",
    "  }\n",
    "\n",
    "  source_ranges = [\"0.0.0.0/0\"]\n",
    "\n",
    "  target_tags = [\"web\"]\n",
    "  description = \"Allow all inbound traffic from any IP on all ports\"\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(config_text):\n",
    "   return f\"\"\"You are a security auditor reviewing the configuration of a Terraform configuration file for security issues.\n",
    "    \n",
    "    Go over the following configuration line-by-line and assess it for:\n",
    "    1. Security misconfigurations\n",
    "    2. Weak or deprecated settings\n",
    "    3. Missing best practices\n",
    "    \n",
    "    ## CONFIGURATION\n",
    "    {config_text}\n",
    "    \n",
    "    Respond with:\n",
    "    - Detected misconfiguration\n",
    "    - Severity (Low/Medium/High)\n",
    "    - Recommended fix\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7q9yZevyEi8b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Configuration Review\n",
       "\n",
       "### Line by Line Analysis:\n",
       "\n",
       "**Line 1:**\n",
       "```hcl\n",
       "provider \"google\" {\n",
       "  project = \"my-project\"\n",
       "  region  = \"us-central1\"\n",
       "}\n",
       "```\n",
       "* **Security Misconfiguration:** The provider is configured to use a specific project ID (\"my-project\") which might be visible in your Terraform state files, potentially exposing this information if shared or committed into version control. However, since this is a common practice when using GCP, unless there's an explicit need to hide the project ID, this isn't considered high risk here.\n",
       "   * **Severity:** Low\n",
       "   * **Recommended Fix:** If sensitive data like project IDs should not be exposed, consider using environment variables or secrets management tools like Vault.\n",
       "\n",
       "**Line 5-7:**\n",
       "```hcl\n",
       "resource \"google_compute_network\" \"default\" {\n",
       "  name                    = \"default-network\"\n",
       "  auto_create_subnetworks = true\n",
       "}\n",
       "```\n",
       "* **Best Practice Violation:** Enabling `auto_create_subnetworks` can lead to unintended resources being created automatically. For example, creating default subnets that could allow unauthorized access if left unmonitored. It's better to manage subnet creation explicitly rather than relying on automatic defaults.\n",
       "   * **Severity:** Medium\n",
       "   * **Recommended Fix:** Set `auto_create_subnetworks` to false and create subnetworks as needed manually. This ensures more controlled resource management.\n",
       "\n",
       "**Lines 9-22:**\n",
       "```hcl\n",
       "resource \"google_compute_firewall\" \"allow_all_inbound\" {\n",
       " ...\n",
       "  source_ranges = [\"0.0.0.0/0\"] # Allows ingress from anywhere\n",
       " ...\n",
       "}\n",
       "```\n",
       "* **Security Misconfiguration:** Allowing all inbound traffic (`source_ranges = \"0.0.0.0/0\"`) exposes services to every possible IP address. Unless intentionally open to the world, this rule poses significant security risks.\n",
       "   * **Severity:** High\n",
       "   * **Recommended Fix:** Restrict `source_ranges` to only necessary IPs/CIDRs instead of allowing all. Use VPC peering, Cloud VPN, or similar mechanisms if you must accept traffic from external networks but limit it to known good sources. Also, ensure that the service tagged with 'web' is properly secured elsewhere (e.g., behind a load balancer).\n",
       "\n",
       "**Additional Notes:**\n",
       "\n",
       "- **Weak or Deprecated Settings:** No deprecated settings found in the provided code snippet. Always check documentation for latest versions and deprecations.\n",
       "  \n",
       "- **Missing Best Practices:**\n",
       "  - Lack of IAM controls: Resource permissions aren’t specified; make sure appropriate roles are assigned via policies.\n",
       "  - Logging & Monitoring: Ensure proper logging and monitoring are set up to detect anomalies (not shown in config).\n",
       "  - Network Segmentation: Not evident whether other segments exist; review overall architecture for segmentation.\n",
       "  - Version Control: Avoid committing credentials or sensitive info into TF files; use secret managers instead.\n",
       "\n",
       "In summary, while some configurations may require adjustments based on specific needs, the identified issues above represent potential vulnerabilities or deviations from best practices."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = inference(make_prompt(config_text))\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
