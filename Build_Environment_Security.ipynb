{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI5ozM2h2Nva"
   },
   "source": [
    "# Build Environment Security Use Case\n",
    "\n",
    "Automate security monitoring and guidance during the software development lifecycle (SDLC) by embedding a GitHub Action that triggers on pull requests (PRs). The system summarizes changes, evaluates them for security risks, and provides actionable recommendations to reviewers. This turns every PR into an opportunity for proactive security posture improvement — not just static scanning, but contextual reasoning.\n",
    "\n",
    "## Model used for this use case\n",
    "Both Instruct Model and Reasoning Model would be suitable for this task. In this example, we used the Instruct Model via SageMaker endpoint.\n",
    "\n",
    "**Note**: Update the configuration variables below to match your deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Update these variables to match your SageMaker deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these variables to match your deployment\n",
    "endpoint_name = 'foundation-sec-8b-endpoint'\n",
    "aws_region = 'us-east-1'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Endpoint: {endpoint_name}\")\n",
    "print(f\"Region: {aws_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPloud0h2LI9"
   },
   "source": [
    "## Setup\n",
    "\n",
    "The setup uses SageMaker endpoint instead of loading the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=aws_region)\n",
    "\n",
    "print(f\"Connected to SageMaker endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation arguments for reproducible outputs\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": None,  # None means deterministic (temperature=0)\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,   # Deterministic sampling\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "\n",
    "print(\"Generation configuration:\")\n",
    "for key, value in generation_args.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, system_prompt):\n",
    "    \"\"\"Inference function using SageMaker endpoint\"\"\"\n",
    "    \n",
    "    # Format the conversation for the model\n",
    "    formatted_prompt = f\"System: {system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant: \"\n",
    "    \n",
    "    # Prepare payload for SageMaker endpoint\n",
    "    payload = {\n",
    "        \"inputs\": formatted_prompt,\n",
    "        \"parameters\": generation_args\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        \n",
    "        # Handle different TGI response formats\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            generated_text = result[0].get('generated_text', '')\n",
    "        elif isinstance(result, dict):\n",
    "            generated_text = result.get('generated_text', str(result))\n",
    "        else:\n",
    "            generated_text = str(result)\n",
    "        \n",
    "        # Clean up the response (remove the original prompt if it's included)\n",
    "        if generated_text.startswith(formatted_prompt):\n",
    "            response_text = generated_text[len(formatted_prompt):].strip()\n",
    "        else:\n",
    "            response_text = generated_text.strip()\n",
    "            \n",
    "        # Remove any trailing special tokens\n",
    "        response_text = re.sub(r'<\\|.*?\\|>$', '', response_text).strip()\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking endpoint: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the inference function\n",
    "test_response = inference(\"Hello, can you help with security code review?\", \"You are a security expert.\")\n",
    "print(\"Test Response:\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a security expert reviewing changes introduced in a pull request.\"\n",
    "\n",
    "def make_prompt(pr_diff_text):\n",
    "    return f'''Analyze the following code diff and identify:\n",
    "    1. Security-relevant changes\n",
    "    2. Any potential vulnerabilities introduced\n",
    "    3. A clear summary of affected areas\n",
    "    4. Recommended remediations (if needed)\n",
    "    \n",
    "    ## PULL REQUEST DIFF\n",
    "    {pr_diff_text}'''\n",
    "\n",
    "print(\"Security analysis functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6zZ0aHC5qC_"
   },
   "source": [
    "## Example: Security-Positive Pull Request Analysis\n",
    "\n",
    "In this sample PR Diff, we replaced use of `eval()` with `ast.literal_eval()` — this is a significant security improvement. `eval()` can execute arbitrary code, whereas `ast.literal_eval()` safely evaluates strings with Python literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_diff_text = \"\"\"\n",
    "diff --git a/app/utils.py b/app/utils.py\n",
    "index e69de29..b25a3c0 100644\n",
    "--- a/app/utils.py\n",
    "+++ b/app/utils.py\n",
    "@@ def process_input(data):\n",
    "-    return eval(data)\n",
    "+    import ast\n",
    "+    return ast.literal_eval(data)\n",
    "\n",
    "diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml\n",
    "index a1b2c3d..d4e5f6g 100644\n",
    "--- a/.github/workflows/deploy.yml\n",
    "+++ b/.github/workflows/deploy.yml\n",
    "@@ steps:\n",
    "-      run: npm run deploy\n",
    "+      run: |\n",
    "+        set -e\n",
    "+        npm run lint\n",
    "+        npm test\n",
    "+        npm run deploy\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample PR diff loaded - this shows a security improvement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SECURITY ANALYSIS OF PULL REQUEST ===\")\n",
    "response = inference(make_prompt(pr_diff_text), SYSTEM_PROMPT)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Analyzing Risky Changes\n",
    "\n",
    "Let's analyze a different PR that introduces potential security risks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risky_pr_diff = \"\"\"\n",
    "diff --git a/app/auth.py b/app/auth.py\n",
    "index abc123..def456 100644\n",
    "--- a/app/auth.py\n",
    "+++ b/app/auth.py\n",
    "@@ def login(username, password):\n",
    "-    query = \"SELECT * FROM users WHERE username = %s AND password = %s\"\n",
    "-    return db.execute(query, (username, password))\n",
    "+    query = f\"SELECT * FROM users WHERE username = '{username}' AND password = '{password}'\"\n",
    "+    return db.execute(query)\n",
    "\n",
    "diff --git a/app/config.py b/app/config.py\n",
    "index 111222..333444 100644\n",
    "--- a/app/config.py\n",
    "+++ b/app/config.py\n",
    "@@ class Config:\n",
    "+    DEBUG = True\n",
    "+    SECRET_KEY = \"hardcoded-secret-key-123\"\n",
    "     DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== ANALYZING RISKY PULL REQUEST ===\")\n",
    "risky_response = inference(make_prompt(risky_pr_diff), SYSTEM_PROMPT)\n",
    "display(Markdown(risky_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Security Analysis\n",
    "\n",
    "Try analyzing your own code diff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own PR diff here for analysis\n",
    "custom_pr_diff = \"\"\"\n",
    "# Paste your git diff output here\n",
    "# Example:\n",
    "# diff --git a/your_file.py b/your_file.py\n",
    "# index abc123..def456 100644\n",
    "# --- a/your_file.py\n",
    "# +++ b/your_file.py\n",
    "# @@ -1,3 +1,4 @@\n",
    "# +import requests\n",
    "#  def fetch_data(url):\n",
    "# -    return \"placeholder\"\n",
    "# +    response = requests.get(url, verify=False)\n",
    "# +    return response.json()\n",
    "\"\"\"\n",
    "\n",
    "if custom_pr_diff.strip() and not custom_pr_diff.startswith(\"# Paste your\"):\n",
    "    print(\"=== ANALYZING YOUR CUSTOM PR ===\")\n",
    "    custom_response = inference(make_prompt(custom_pr_diff), SYSTEM_PROMPT)\n",
    "    display(Markdown(custom_response))\n",
    "else:\n",
    "    print(\"Add your PR diff in the cell above to analyze it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Multi-Factor Security Analysis\n",
    "\n",
    "Let's create a more comprehensive analysis that looks at multiple security aspects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_security_analysis(pr_diff_text):\n",
    "    \"\"\"Perform multi-aspect security analysis\"\"\"\n",
    "    \n",
    "    analyses = {\n",
    "        \"Vulnerability Assessment\": {\n",
    "            \"prompt\": f\"Focus on identifying potential vulnerabilities in this code diff. Look for injection flaws, authentication bypasses, insecure configurations, and data exposure risks:\\n\\n{pr_diff_text}\",\n",
    "            \"system\": \"You are a vulnerability assessment specialist.\"\n",
    "        },\n",
    "        \"Supply Chain Security\": {\n",
    "            \"prompt\": f\"Analyze this diff for supply chain security risks including new dependencies, build process changes, and third-party integrations:\\n\\n{pr_diff_text}\",\n",
    "            \"system\": \"You are a supply chain security expert.\"\n",
    "        },\n",
    "        \"DevSecOps Impact\": {\n",
    "            \"prompt\": f\"Evaluate how these changes impact the overall DevSecOps pipeline, CI/CD security, and deployment practices:\\n\\n{pr_diff_text}\",\n",
    "            \"system\": \"You are a DevSecOps security consultant.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for analysis_type, config in analyses.items():\n",
    "        print(f\"\\n=== {analysis_type.upper()} ===\")\n",
    "        response = inference(config[\"prompt\"], config[\"system\"])\n",
    "        results[analysis_type] = response\n",
    "        display(Markdown(f\"### {analysis_type}\\n{response}\"))\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Comprehensive security analysis function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive analysis on the risky PR\n",
    "print(\"=== COMPREHENSIVE SECURITY ANALYSIS ===\")\n",
    "comprehensive_results = comprehensive_security_analysis(risky_pr_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Score Generator\n",
    "\n",
    "Generate a security score and recommendation for the PR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_security_score(pr_diff_text):\n",
    "    \"\"\"Generate a security score and recommendation for the PR\"\"\"\n",
    "    \n",
    "    score_prompt = f\"\"\"Analyze this pull request diff and provide:\n",
    "    \n",
    "    1. A security score from 1-10 (1 = high risk, 10 = excellent security)\n",
    "    2. A brief risk assessment\n",
    "    3. A clear recommendation (APPROVE, REQUEST CHANGES, or BLOCK)\n",
    "    4. Top 3 action items for the developer\n",
    "    \n",
    "    Format your response with clear sections and be decisive in your recommendation.\n",
    "    \n",
    "    PULL REQUEST DIFF:\n",
    "    {pr_diff_text}\"\"\"\n",
    "    \n",
    "    score_system = \"You are a senior security engineer providing a final security assessment for code review.\"\n",
    "    \n",
    "    return inference(score_prompt, score_system)\n",
    "\n",
    "print(\"=== SECURITY SCORE FOR RISKY PR ===\")\n",
    "security_score = generate_security_score(risky_pr_diff)\n",
    "display(Markdown(security_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub Integration Example\n",
    "\n",
    "Here's how you could structure this for GitHub Actions integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_pr_security_review(pr_diff, pr_number, repo_name):\n",
    "    \"\"\"Complete security review function for GitHub integration\"\"\"\n",
    "    \n",
    "    print(f\"🔍 Security Review for PR #{pr_number} in {repo_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic security analysis\n",
    "    basic_analysis = inference(make_prompt(pr_diff), SYSTEM_PROMPT)\n",
    "    \n",
    "    # Security score\n",
    "    score_result = generate_security_score(pr_diff)\n",
    "    \n",
    "    # Format for GitHub comment\n",
    "    github_comment = f\"\"\"## 🛡️ Security Review Results\n",
    "\n",
    "### Analysis Summary\n",
    "{basic_analysis}\n",
    "\n",
    "---\n",
    "\n",
    "### Security Assessment\n",
    "{score_result}\n",
    "\n",
    "---\n",
    "*This review was generated by the Cisco Foundation Security Model*\n",
    "\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": basic_analysis,\n",
    "        \"score\": score_result,\n",
    "        \"github_comment\": github_comment\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "review_result = github_pr_security_review(pr_diff_text, \"123\", \"myorg/myrepo\")\n",
    "print(\"\\n=== GITHUB COMMENT FORMAT ===\")\n",
    "display(Markdown(review_result[\"github_comment\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
